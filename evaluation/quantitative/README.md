# Performance Evaluation

This directory contains scripts, test payloads and configuration files used
to reproduce the performance evaluation described in our paper. While the
main text of the paper summarises the evaluation setup and
results, this README provides complete instructions for running the load
tests, collecting metrics and interpreting the data.

## Repository structure

The directory is organised as follows:

```text
evaluation/quantitative/
├── README.md
├── evidence-provider/
│   ├── k6-evidence-infra.js
│   ├── k6-evidence-quote.js
│   ├── k6-evidence-workload.js
│   └── test-payloads/
│       └── challenge.json  # JSON body used for evidence collection
├── evidence-verifier/
│   ├── k6-verify-infra.js
│   ├── k6-verify-tdx.js
│   ├── k6-verify-workloads.js
│   └── test-payloads/
│       ├── infra.json      # JSON body used for verifying infrastructure evidence
│       ├── quote.json      # JSON body used for verifying TDX quotes
│       └── workloads.json  # JSON body used for verifying workload evidence
├── prometheus.yml          # Prometheus scrape config
├── results/
└── scripts/
  ├── set-up-vm.sh                # Installs Docker, runs verifier container, fetches payloads
  ├── prepare-load-tests.sh       # Installs/updates k6, downloads scripts and Prometheus config
  ├── run-load-tests.sh           # Executes k6 load tests and records timing information
  └── export-prometheus-results.sh
```

Each `k6-*.js` file is a load test written in JavaScript using the Grafana k6
framework. The tests in `evidence-provider` target the in-CVM Evidence Provider via
an NGINX middleware deployed on Cloud Run, whereas the tests in `evidence-verifier` invoke
the endpoints of the on-client running Evidence Verifier service. Test payloads
under `test-payloads/` are complete request bodies used by the tests.
The Prometheus configuration in `prometheus.yml` defines scrape targets for
scraping metrics from the services under test.

## Overview of the evaluation setup

The evaluation measures the attestation latency introduced by TCS in two scenarios: (1) gathering attestation evidence from CVMs (the Evidence Provider), and (2) verifying that evidence on
the client device (the Evidence Verifier). To minimise noise from network
and platform variability, the tests instrument the controllers in each
service with Prometheus histograms. Requests are generated by k6 scripts
running on client VMs sized _small_ (2 vCPU/2 GB), _medium_ (2 vCPU/8 GB) and
_large_ (8 vCPU/32 GB). Each client downloads scripts and payloads, then
executes the load test suite to exercise the Evidence Provider and Evidence Verifier services. The
instrumented services expose metrics on `/metrics`, which are scraped by a
local Prometheus instance. After the suite completes, the metrics and
histograms are exported and summarised.

Each client VM talks to the NGINX middleware (acting as an untrusted API gateway and load balancer)
to reach the Evidence Provider instances in different CVMs. At
the same time, a containerised Evidence Verifier runs locally on the client
to perform verification. The tests aim to model realistic usage where
clients periodically trigger verification operations, rather than saturating
the services with unrealistic concurrency.

## Client VM provisioning

Before running any tests, provision three VMs in Google Compute Engine. The
following commands create the small, medium and large clients used in our
evaluation. Adjust zone or machine type if needed:

```bash
# Small client (E2, 2 vCPU, 2 GB RAM)
gcloud compute instances create client-sm \
  --zone "europe-west4-a" \
  --machine-type "e2-small" \
  --tags "client" \
  --image-family "ubuntu-2204-lts" --image-project "ubuntu-os-cloud" \
  --boot-disk-size "20GB" --boot-disk-type "pd-balanced"

# Medium client (E2, 2 vCPU, 8 GB RAM)
gcloud compute instances create client-md \
  --zone "europe-west4-a" \
  --machine-type "e2-standard-2" \
  --tags "client" \
  --image-family "ubuntu-2204-lts" --image-project "ubuntu-os-cloud" \
  --boot-disk-size "20GB" --boot-disk-type "pd-balanced"

# Large client (E2, 8 vCPU, 32 GB RAM)
gcloud compute instances create client-lg \
  --zone "europe-west4-a" \
  --machine-type "e2-standard-8" \
  --tags "client" \
  --image-family "ubuntu-2204-lts" --image-project "ubuntu-os-cloud" \
  --boot-disk-size "20GB" --boot-disk-type "pd-balanced"
```

After the VMs are running and reachable via SSH, install Docker and the
Evidence Verifier service by running `set-up-vm.sh` on each VM:

```bash
scp evaluation/quantitative/scripts/set-up-vm.sh client-sm:~
ssh client-sm 'bash ~/set-up-vm.sh'
```

The script performs the following steps:

- Removes old container runtimes (if any) and installs Docker.
- Pulls and runs the `evidenceverifier` container on port `8081`.
- Downloads test payloads for verifying infrastructure, quote and workloads evidence, as well as the challenge payload for gathering evidence via the Evidence Provider.
- Creates a helper `env.sh` and README with manual curl commands for sanity checking the endpoints.

Running the script on each client ensures that the Evidence Verifier is
always running fresh and that payloads are up to date.

## Preparing load tests and Prometheus

Next, run `prepare-load-tests.sh` on each client. This script:

- Installs or updates k6 to the latest release via the GitHub API.
- Downloads the k6 scripts and payloads under `evaluation/quantitative`.
- Downloads a Prometheus configuration (`prometheus.yml`) from the repository.
- Starts a Prometheus container with the downloaded configuration.

The configuration scrapes:

- The three in-CVM Evidence Providers through NGINX (`evidenceprovider` job).
- The local Evidence Verifier on `localhost:8081` (`evidenceverifier` job).

The `prometheus.yml` included in the repository hard‑codes the IP addresses
of the CVMs and the local Evidence Verifier port. If you change
the deployment topology, update this file accordingly. The script always
restarts Prometheus to ensure a clean time series database (TSDB) for each
run.

## Running the load test suite

Execute the full suite with `run-load-tests.sh`. This script runs each k6
test sequentially and records start/end timestamps to correlate with
Prometheus data. By default it resets Prometheus and leaves the
Evidence Verifier container running. Use `--no-reset-prom` if you want to
add test runs to the same TSDB (not recommended for the evaluation).

Example usage on the small client:

```bash
scp evaluation/quantitative/scripts/run-load-tests.sh client-sm:~
ssh client-sm 'bash ~/run-load-tests.sh --client client-sm'
```

The script performs the following steps:

- Pre-flight: checks for k6 and `prometheus.yml` and ensures Prometheus is healthy.
- Run directory setup: generates a unique run ID (timestamp and client name) and creates a results folder under `~/results/<RUN_ID>` with subdirectories `k6/`, `prometheus/` and `notes/`.
- Metadata recording: writes VM information and version metadata (Docker, k6, Prometheus) to `notes/` for reproducibility.
- Provider tests: runs the Evidence Provider k6 scripts against the middleware endpoint.
- Verifier tests: runs the Evidence Verifier k6 scripts against the local container.
- Timing CSV: records the start/end times of each test in `notes/test_times.csv` so that the export script knows which window to query.
- Delays: sleeps for a configurable period (`SCRAPE_WAIT`, default `20 s`) between tests to allow Prometheus to scrape the latest data.

You can comment out individual `run_one` calls in the script when debugging
a particular endpoint. The final lines of the script remind you to run the
export script to capture metrics.

## Exporting and summarising Prometheus metrics

After the suite has completed, run `export-prometheus-results.sh` to
snapshot the TSDB and export selected PromQL queries. The script expects you
to set `RESULTS_DIR` to the run directory created by `run-load-tests.sh`.
Example:

```bash
ssh client-sm 'export RESULTS_DIR=~/results/20251224T130431Z-client-sm && \
               bash ~/export-prometheus-results.sh'
```

The export script performs the following:

- Creates a TSDB snapshot via Prometheus’s admin API and copies it out of the container into `prometheus/tsdb_snapshot/`.
- Exports Prometheus status information and flags for reproducibility.
- Reads the `test_times.csv` file from the run and executes PromQL queries for each test window:
  - P50 and P95 request latencies (histogram quantiles) per endpoint.
  - Average latency (mean of histogram) per endpoint.
  - Request rate (RPS) per endpoint.
  - Approximate total requests per endpoint (using `increase()`).
- Writes the query results as JSON under `prometheus/promql/range/`.

These exported metrics form the basis for the numbers in the tables in the
paper. The per‑endpoint histograms in the snapshot can also be loaded into a
Prometheus or Grafana instance for interactive exploration.

## Adapting the k6 tests

The default k6 scripts generate moderate load that should not stress the
services excessively. You can adapt the scripts to explore higher rates or
longer durations:

- Evidence Provider tests use four stages up to 50 virtual users (VUs). Reduce the target or duration if the client VM cannot sustain this.
- Evidence Verifier tests use constant arrival rates to simulate periodic verification at different frequencies. Adjust the rate and duration to probe the throughput that each client can handle.

Always ensure that the Prometheus scrape interval (15 s by default) and the
export window (`RATE_WIN`) are appropriate for your new test durations.

## Interpreting the results

The `results/` directory contains everything needed to interpret a test run:

- `notes/vm_info.txt`: VM hardware details and OS information.
- `notes/versions.txt`: versions of k6, Docker, Prometheus and the Evidence Verifier container.
- `k6/*.json`: summary statistics from each k6 test, including request counts, failure rates and client‑observed latencies.
- `prometheus/promql/range/`: JSON files containing time-series data for each endpoint’s P50, P95, average latency, request rate and approximate total requests.

For each client size in the paper we report the median (P50) and 95th
percentile (P95) latencies of the Evidence Provider and Evidence Verifier controllers (derived
from Prometheus) alongside the client‑observed latencies from k6. We also
report RPS and total requests to show how busy the services were during the
window. These numbers feed into the performance evaluation section and are
summarised in tables in the paper. Use the exported JSON to recompute
alternate quantiles or aggregations if needed.
